{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from example_lorenz import get_lorenz_data\n",
    "from sindy_utils import library_size\n",
    "from training import train_network\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # generate training, validation, testing data\n",
    "# noise_strength = 1e-6\n",
    "# training_data = get_lorenz_data(100, noise_strength=noise_strength) #1024\n",
    "# validation_data = get_lorenz_data(10, noise_strength=noise_strength) #20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert data to tensors and move to GPU\n",
    "def convert_to_tensors(data_dict, device):\n",
    "    for key, value in data_dict.items():\n",
    "        if value is not None:\n",
    "            data_dict[key] = torch.tensor(value).float().to(device)\n",
    "    return data_dict\n",
    "\n",
    "# Generate training, validation data\n",
    "noise_strength = 1e-6\n",
    "training_data = get_lorenz_data(1024, noise_strength=noise_strength) # 1024\n",
    "validation_data = get_lorenz_data(20, noise_strength=noise_strength) # 20\n",
    "\n",
    "# Specify the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Convert training and validation data to tensors and move to GPU\n",
    "training_data = convert_to_tensors(training_data, device)\n",
    "validation_data = convert_to_tensors(validation_data, device)\n",
    "\n",
    "# If 'ddx' is required and should be set to None\n",
    "training_data['ddx'] = None\n",
    "validation_data['ddx'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up model and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "\n",
    "params['input_dim'] = 128\n",
    "params['latent_dim'] = 3\n",
    "params['model_order'] = 1\n",
    "params['poly_order'] = 3\n",
    "params['include_sine'] = False\n",
    "params['library_dim'] = library_size(params['latent_dim'], params['poly_order'], params['include_sine'], True)\n",
    "\n",
    "# sequential thresholding parameters\n",
    "params['sequential_thresholding'] = True\n",
    "params['coefficient_threshold'] = 0.1\n",
    "params['threshold_frequency'] = 1\n",
    "params['coefficient_mask'] = torch.ones((params['library_dim'], params['latent_dim']))\n",
    "#print('params[coefficient_mask]:', params['coefficient_mask'].shape)\n",
    "params['coefficient_initialization'] = 'constant'\n",
    "\n",
    "# loss function weighting\n",
    "params['loss_weight_decoder'] = 1.0\n",
    "params['loss_weight_sindy_z'] = 0.0\n",
    "params['loss_weight_sindy_x'] = 1e-4\n",
    "params['loss_weight_sindy_regularization'] = 1e-5\n",
    "\n",
    "params['activation'] = 'sigmoid'\n",
    "params['widths'] = [64,32]\n",
    "\n",
    "# training parameters\n",
    "params['epoch_size'] = training_data['x'].shape[0]\n",
    "print(params['epoch_size'])\n",
    "params['batch_size'] = 1024 #1024\n",
    "params['learning_rate'] = 1e-3\n",
    "\n",
    "params['data_path'] = os.getcwd() + '/'\n",
    "params['print_progress'] = True\n",
    "params['print_frequency'] = 1\n",
    "\n",
    "# training time cutoffs\n",
    "params['max_epochs'] = 5001 #5001\n",
    "params['refinement_epochs'] = 1001 #1001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 0\n",
      "Sequential thresholding is enabled\n",
      "val_data None\n",
      "TRAINING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhaox\\OneDrive - Delft University of Technology\\桌面\\Q4\\reproductory\\sindy_reproduction\\sindy_reproduction\\example\\lorenz\\../..\\training.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'x': torch.tensor(data['x'][idxs], dtype=torch.float32).to(device),\n",
      "c:\\Users\\zhaox\\OneDrive - Delft University of Technology\\桌面\\Q4\\reproductory\\sindy_reproduction\\sindy_reproduction\\example\\lorenz\\../..\\training.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'dx': torch.tensor(data['dx'][idxs], dtype=torch.float32).to(device)\n",
      "c:\\Users\\zhaox\\OneDrive - Delft University of Technology\\桌面\\Q4\\reproductory\\sindy_reproduction\\sindy_reproduction\\example\\lorenz\\../..\\training.py:234: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_dict['coefficient_mask'] = torch.tensor(params['coefficient_mask'], dtype=torch.float32).to(device)\n",
      "c:\\Users\\zhaox\\OneDrive - Delft University of Technology\\桌面\\Q4\\reproductory\\sindy_reproduction\\sindy_reproduction\\example\\lorenz\\../..\\training.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_tensor = torch.tensor(val_data['x'], dtype=torch.float32).to(device)\n",
      "c:\\Users\\zhaox\\OneDrive - Delft University of Technology\\桌面\\Q4\\reproductory\\sindy_reproduction\\sindy_reproduction\\example\\lorenz\\../..\\training.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_data['dx'] = torch.tensor(val_data['dx'], dtype=torch.float32).to(device)\n",
      "c:\\Users\\zhaox\\OneDrive - Delft University of Technology\\桌面\\Q4\\reproductory\\sindy_reproduction\\sindy_reproduction\\example\\lorenz\\../..\\training.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sindy_model_terms = [torch.sum(torch.tensor(params['coefficient_mask'])).cpu().numpy()]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "   Training Losses:\n",
      "      Total Loss: 0.10385086387395859\n",
      "      decoder Loss: 0.10238063335418701\n",
      "      sindy_z Loss: 55.16654586791992\n",
      "      sindy_x Loss: 14.604644775390625\n",
      "      sindy_regularization Loss: 0.9771175384521484\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.09781479090452194\n",
      "      decoder Loss: 0.09678245335817337\n",
      "      sindy_z Loss: 51.758419036865234\n",
      "      sindy_x Loss: 10.225693702697754\n",
      "      sindy_regularization Loss: 0.9771175384521484\n",
      "Decoder Loss Ratio: 0.527618, Decoder SINDy Loss Ratio: 1.000746\n",
      "Epoch 1\n",
      "   Training Losses:\n",
      "      Total Loss: 0.0988842323422432\n",
      "      decoder Loss: 0.09741421788930893\n",
      "      sindy_z Loss: 60.327178955078125\n",
      "      sindy_x Loss: 14.604752540588379\n",
      "      sindy_regularization Loss: 0.9538058042526245\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.09159348160028458\n",
      "      decoder Loss: 0.09056136012077332\n",
      "      sindy_z Loss: 52.64815139770508\n",
      "      sindy_x Loss: 10.225862503051758\n",
      "      sindy_regularization Loss: 0.9538058042526245\n",
      "Decoder Loss Ratio: 0.493703, Decoder SINDy Loss Ratio: 1.000763\n",
      "Epoch 2\n",
      "   Training Losses:\n",
      "      Total Loss: 0.09786788374185562\n",
      "      decoder Loss: 0.09639699012041092\n",
      "      sindy_z Loss: 144.5647430419922\n",
      "      sindy_x Loss: 14.615866661071777\n",
      "      sindy_regularization Loss: 0.9304512143135071\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.09079074114561081\n",
      "      decoder Loss: 0.08975773304700851\n",
      "      sindy_z Loss: 119.37083435058594\n",
      "      sindy_x Loss: 10.236998558044434\n",
      "      sindy_regularization Loss: 0.9304512143135071\n",
      "Decoder Loss Ratio: 0.489322, Decoder SINDy Loss Ratio: 1.001853\n",
      "Epoch 3\n",
      "   Training Losses:\n",
      "      Total Loss: 0.09508281201124191\n",
      "      decoder Loss: 0.09360173344612122\n",
      "      sindy_z Loss: 706.2140502929688\n",
      "      sindy_x Loss: 14.720597267150879\n",
      "      sindy_regularization Loss: 0.9026193022727966\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.08771754801273346\n",
      "      decoder Loss: 0.08667560666799545\n",
      "      sindy_z Loss: 574.1041870117188\n",
      "      sindy_x Loss: 10.329158782958984\n",
      "      sindy_regularization Loss: 0.9026193022727966\n",
      "Decoder Loss Ratio: 0.472520, Decoder SINDy Loss Ratio: 1.010872\n",
      "Epoch 4\n",
      "   Training Losses:\n",
      "      Total Loss: 0.08476795256137848\n",
      "      decoder Loss: 0.08317685127258301\n",
      "      sindy_z Loss: 4943.7705078125\n",
      "      sindy_x Loss: 15.825188636779785\n",
      "      sindy_regularization Loss: 0.8583329319953918\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.07694685459136963\n",
      "      decoder Loss: 0.0758124515414238\n",
      "      sindy_z Loss: 4042.002685546875\n",
      "      sindy_x Loss: 11.258212089538574\n",
      "      sindy_regularization Loss: 0.8583329319953918\n",
      "Decoder Loss Ratio: 0.413298, Decoder SINDy Loss Ratio: 1.101795\n",
      "Epoch 5\n",
      "   Training Losses:\n",
      "      Total Loss: 0.06439720094203949\n",
      "      decoder Loss: 0.06214188411831856\n",
      "      sindy_z Loss: 20385.603515625\n",
      "      sindy_x Loss: 22.47300910949707\n",
      "      sindy_regularization Loss: 0.8017604947090149\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.055841222405433655\n",
      "      decoder Loss: 0.05413439869880676\n",
      "      sindy_z Loss: 17183.111328125\n",
      "      sindy_x Loss: 16.988080978393555\n",
      "      sindy_regularization Loss: 0.8017604947090149\n",
      "Decoder Loss Ratio: 0.295119, Decoder SINDy Loss Ratio: 1.662553\n",
      "Epoch 6\n",
      "   Training Losses:\n",
      "      Total Loss: 0.052680790424346924\n",
      "      decoder Loss: 0.05022519826889038\n",
      "      sindy_z Loss: 12194.50390625\n",
      "      sindy_x Loss: 24.48128890991211\n",
      "      sindy_regularization Loss: 0.7460176944732666\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.04387134313583374\n",
      "      decoder Loss: 0.04195098206400871\n",
      "      sindy_z Loss: 11159.0869140625\n",
      "      sindy_x Loss: 19.128978729248047\n",
      "      sindy_regularization Loss: 0.7460176944732666\n",
      "Decoder Loss Ratio: 0.228700, Decoder SINDy Loss Ratio: 1.872074\n",
      "Epoch 7\n",
      "   Training Losses:\n",
      "      Total Loss: 0.04586891829967499\n",
      "      decoder Loss: 0.043895281851291656\n",
      "      sindy_z Loss: 1733.9580078125\n",
      "      sindy_x Loss: 19.665285110473633\n",
      "      sindy_regularization Loss: 0.71073317527771\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.03701092302799225\n",
      "      decoder Loss: 0.035500120371580124\n",
      "      sindy_z Loss: 1732.83251953125\n",
      "      sindy_x Loss: 15.036931037902832\n",
      "      sindy_regularization Loss: 0.71073317527771\n",
      "Decoder Loss Ratio: 0.193532, Decoder SINDy Loss Ratio: 1.471602\n",
      "Epoch 8\n",
      "   Training Losses:\n",
      "      Total Loss: 0.042364269495010376\n",
      "      decoder Loss: 0.04046175256371498\n",
      "      sindy_z Loss: 1025.49560546875\n",
      "      sindy_x Loss: 18.956314086914062\n",
      "      sindy_regularization Loss: 0.6885133385658264\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.03348739817738533\n",
      "      decoder Loss: 0.03204551339149475\n",
      "      sindy_z Loss: 1019.24267578125\n",
      "      sindy_x Loss: 14.349998474121094\n",
      "      sindy_regularization Loss: 0.6885133385658264\n",
      "Decoder Loss Ratio: 0.174699, Decoder SINDy Loss Ratio: 1.404375\n",
      "Epoch 9\n",
      "   Training Losses:\n",
      "      Total Loss: 0.040750108659267426\n",
      "      decoder Loss: 0.03892131894826889\n",
      "      sindy_z Loss: 654.495361328125\n",
      "      sindy_x Loss: 18.22089195251465\n",
      "      sindy_regularization Loss: 0.6701793670654297\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.031896546483039856\n",
      "      decoder Loss: 0.030523234978318214\n",
      "      sindy_z Loss: 647.5077514648438\n",
      "      sindy_x Loss: 13.666109085083008\n",
      "      sindy_regularization Loss: 0.6701793670654297\n",
      "Decoder Loss Ratio: 0.166400, Decoder SINDy Loss Ratio: 1.337445\n",
      "Epoch 10\n",
      "   Training Losses:\n",
      "      Total Loss: 0.03985658288002014\n",
      "      decoder Loss: 0.0380789153277874\n",
      "      sindy_z Loss: 501.7652282714844\n",
      "      sindy_x Loss: 17.711193084716797\n",
      "      sindy_regularization Loss: 0.6547812223434448\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.031053218990564346\n",
      "      decoder Loss: 0.02972663752734661\n",
      "      sindy_z Loss: 487.5475158691406\n",
      "      sindy_x Loss: 13.200335502624512\n",
      "      sindy_regularization Loss: 0.6547812223434448\n",
      "Decoder Loss Ratio: 0.162057, Decoder SINDy Loss Ratio: 1.291862\n",
      "Epoch 11\n",
      "   Training Losses:\n",
      "      Total Loss: 0.03921104967594147\n",
      "      decoder Loss: 0.03747302293777466\n",
      "      sindy_z Loss: 413.4913024902344\n",
      "      sindy_x Loss: 17.316099166870117\n",
      "      sindy_regularization Loss: 0.6414703726768494\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.03047170303761959\n",
      "      decoder Loss: 0.02918015606701374\n",
      "      sindy_z Loss: 392.72039794921875\n",
      "      sindy_x Loss: 12.851323127746582\n",
      "      sindy_regularization Loss: 0.6414703726768494\n",
      "Decoder Loss Ratio: 0.159078, Decoder SINDy Loss Ratio: 1.257706\n",
      "Epoch 12\n",
      "   Training Losses:\n",
      "      Total Loss: 0.038640670478343964\n",
      "      decoder Loss: 0.03693438321352005\n",
      "      sindy_z Loss: 357.77606201171875\n",
      "      sindy_x Loss: 16.99993133544922\n",
      "      sindy_regularization Loss: 0.6295998096466064\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.02997739054262638\n",
      "      decoder Loss: 0.02871483750641346\n",
      "      sindy_z Loss: 332.27154541015625\n",
      "      sindy_x Loss: 12.562569618225098\n",
      "      sindy_regularization Loss: 0.6295998096466064\n",
      "Decoder Loss Ratio: 0.156542, Decoder SINDy Loss Ratio: 1.229447\n",
      "Epoch 13\n",
      "   Training Losses:\n",
      "      Total Loss: 0.038010697811841965\n",
      "      decoder Loss: 0.03633428364992142\n",
      "      sindy_z Loss: 317.4621276855469\n",
      "      sindy_x Loss: 16.702255249023438\n",
      "      sindy_regularization Loss: 0.6186660528182983\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.02943059615790844\n",
      "      decoder Loss: 0.028195375576615334\n",
      "      sindy_z Loss: 290.5327453613281\n",
      "      sindy_x Loss: 12.290350914001465\n",
      "      sindy_regularization Loss: 0.6186660528182983\n",
      "Decoder Loss Ratio: 0.153710, Decoder SINDy Loss Ratio: 1.202806\n",
      "Epoch 14\n",
      "   Training Losses:\n",
      "      Total Loss: 0.037273749709129333\n",
      "      decoder Loss: 0.03562319278717041\n",
      "      sindy_z Loss: 292.5062561035156\n",
      "      sindy_x Loss: 16.444732666015625\n",
      "      sindy_regularization Loss: 0.6082776188850403\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.02879181317985058\n",
      "      decoder Loss: 0.027581002563238144\n",
      "      sindy_z Loss: 266.319091796875\n",
      "      sindy_x Loss: 12.04726791381836\n",
      "      sindy_regularization Loss: 0.6082776188850403\n",
      "Decoder Loss Ratio: 0.150360, Decoder SINDy Loss Ratio: 1.179016\n",
      "Epoch 15\n",
      "   Training Losses:\n",
      "      Total Loss: 0.03638344258069992\n",
      "      decoder Loss: 0.03475626930594444\n",
      "      sindy_z Loss: 278.61700439453125\n",
      "      sindy_x Loss: 16.211894989013672\n",
      "      sindy_regularization Loss: 0.5982056856155396\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.028024230152368546\n",
      "      decoder Loss: 0.026836300268769264\n",
      "      sindy_z Loss: 254.34967041015625\n",
      "      sindy_x Loss: 11.819479942321777\n",
      "      sindy_regularization Loss: 0.5982056856155396\n",
      "Decoder Loss Ratio: 0.146301, Decoder SINDy Loss Ratio: 1.156724\n",
      "Epoch 16\n",
      "   Training Losses:\n",
      "      Total Loss: 0.03530559316277504\n",
      "      decoder Loss: 0.0336986668407917\n",
      "      sindy_z Loss: 271.3382263183594\n",
      "      sindy_x Loss: 16.01045799255371\n",
      "      sindy_regularization Loss: 0.5883527398109436\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.027103416621685028\n",
      "      decoder Loss: 0.02593579702079296\n",
      "      sindy_z Loss: 249.5960693359375\n",
      "      sindy_x Loss: 11.617363929748535\n",
      "      sindy_regularization Loss: 0.5883527398109436\n",
      "Decoder Loss Ratio: 0.141391, Decoder SINDy Loss Ratio: 1.136943\n",
      "Epoch 17\n",
      "   Training Losses:\n",
      "      Total Loss: 0.03401538357138634\n",
      "      decoder Loss: 0.03242548927664757\n",
      "      sindy_z Loss: 259.70465087890625\n",
      "      sindy_x Loss: 15.841049194335938\n",
      "      sindy_regularization Loss: 0.5787526369094849\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.02601672150194645\n",
      "      decoder Loss: 0.024866215884685516\n",
      "      sindy_z Loss: 239.96217346191406\n",
      "      sindy_x Loss: 11.44719123840332\n",
      "      sindy_regularization Loss: 0.5787526369094849\n",
      "Decoder Loss Ratio: 0.135560, Decoder SINDy Loss Ratio: 1.120289\n",
      "Epoch 18\n",
      "   Training Losses:\n",
      "      Total Loss: 0.03236529231071472\n",
      "      decoder Loss: 0.030797382816672325\n",
      "      sindy_z Loss: 211.2275848388672\n",
      "      sindy_x Loss: 15.622149467468262\n",
      "      sindy_regularization Loss: 0.5696998238563538\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.024632848799228668\n",
      "      decoder Loss: 0.023504512384533882\n",
      "      sindy_z Loss: 193.96026611328125\n",
      "      sindy_x Loss: 11.22638988494873\n",
      "      sindy_regularization Loss: 0.5696998238563538\n",
      "Decoder Loss Ratio: 0.128137, Decoder SINDy Loss Ratio: 1.098680\n",
      "Epoch 19\n",
      "   Training Losses:\n",
      "      Total Loss: 0.029517265036702156\n",
      "      decoder Loss: 0.027970338240265846\n",
      "      sindy_z Loss: 124.76318359375\n",
      "      sindy_x Loss: 15.413090705871582\n",
      "      sindy_regularization Loss: 0.5617910027503967\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.02217205986380577\n",
      "      decoder Loss: 0.021065233275294304\n",
      "      sindy_z Loss: 111.56199645996094\n",
      "      sindy_x Loss: 11.01209545135498\n",
      "      sindy_regularization Loss: 0.5617910027503967\n",
      "Decoder Loss Ratio: 0.114839, Decoder SINDy Loss Ratio: 1.077708\n",
      "Epoch 20\n",
      "   Training Losses:\n",
      "      Total Loss: 0.024789419025182724\n",
      "      decoder Loss: 0.023178620263934135\n",
      "      sindy_z Loss: 101.47224426269531\n",
      "      sindy_x Loss: 16.052574157714844\n",
      "      sindy_regularization Loss: 0.5541275143623352\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.01805557869374752\n",
      "      decoder Loss: 0.01689385250210762\n",
      "      sindy_z Loss: 91.57688903808594\n",
      "      sindy_x Loss: 11.561854362487793\n",
      "      sindy_regularization Loss: 0.5541275143623352\n",
      "Decoder Loss Ratio: 0.092098, Decoder SINDy Loss Ratio: 1.131511\n",
      "Epoch 21\n",
      "   Training Losses:\n",
      "      Total Loss: 0.021229203790426254\n",
      "      decoder Loss: 0.019545095041394234\n",
      "      sindy_z Loss: 108.54012298583984\n",
      "      sindy_x Loss: 16.78668975830078\n",
      "      sindy_regularization Loss: 0.5438439846038818\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.015197046101093292\n",
      "      decoder Loss: 0.013972149230539799\n",
      "      sindy_z Loss: 100.42694091796875\n",
      "      sindy_x Loss: 12.194588661193848\n",
      "      sindy_regularization Loss: 0.5438439846038818\n",
      "Decoder Loss Ratio: 0.076170, Decoder SINDy Loss Ratio: 1.193434\n",
      "Epoch 22\n",
      "   Training Losses:\n",
      "      Total Loss: 0.019268203526735306\n",
      "      decoder Loss: 0.01760905422270298\n",
      "      sindy_z Loss: 92.3176040649414\n",
      "      sindy_x Loss: 16.53814125061035\n",
      "      sindy_regularization Loss: 0.5334269404411316\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.013808985240757465\n",
      "      decoder Loss: 0.012613256461918354\n",
      "      sindy_z Loss: 84.2647476196289\n",
      "      sindy_x Loss: 11.90394115447998\n",
      "      sindy_regularization Loss: 0.5334269404411316\n",
      "Decoder Loss Ratio: 0.068762, Decoder SINDy Loss Ratio: 1.164989\n",
      "Epoch 23\n",
      "   Training Losses:\n",
      "      Total Loss: 0.01790548861026764\n",
      "      decoder Loss: 0.016283472999930382\n",
      "      sindy_z Loss: 85.04749298095703\n",
      "      sindy_x Loss: 16.16765022277832\n",
      "      sindy_regularization Loss: 0.5250473022460938\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.012856968678534031\n",
      "      decoder Loss: 0.01170373149216175\n",
      "      sindy_z Loss: 76.79351806640625\n",
      "      sindy_x Loss: 11.479860305786133\n",
      "      sindy_regularization Loss: 0.5250473022460938\n",
      "Decoder Loss Ratio: 0.063804, Decoder SINDy Loss Ratio: 1.123486\n",
      "Epoch 24\n",
      "   Training Losses:\n",
      "      Total Loss: 0.016871625557541847\n",
      "      decoder Loss: 0.01529665756970644\n",
      "      sindy_z Loss: 75.47132110595703\n",
      "      sindy_x Loss: 15.697839736938477\n",
      "      sindy_regularization Loss: 0.5184438228607178\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.012159902602434158\n",
      "      decoder Loss: 0.011053571477532387\n",
      "      sindy_z Loss: 65.93057250976562\n",
      "      sindy_x Loss: 11.011467933654785\n",
      "      sindy_regularization Loss: 0.5184438228607178\n",
      "Decoder Loss Ratio: 0.060260, Decoder SINDy Loss Ratio: 1.077647\n",
      "Epoch 25\n",
      "   Training Losses:\n",
      "      Total Loss: 0.016043702140450478\n",
      "      decoder Loss: 0.014492263086140156\n",
      "      sindy_z Loss: 71.48046112060547\n",
      "      sindy_x Loss: 15.463077545166016\n",
      "      sindy_regularization Loss: 0.5131231546401978\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.011610615998506546\n",
      "      decoder Loss: 0.010529227554798126\n",
      "      sindy_z Loss: 61.01006317138672\n",
      "      sindy_x Loss: 10.7625732421875\n",
      "      sindy_regularization Loss: 0.5131231546401978\n",
      "Decoder Loss Ratio: 0.057401, Decoder SINDy Loss Ratio: 1.053288\n",
      "Epoch 26\n",
      "   Training Losses:\n",
      "      Total Loss: 0.015336766839027405\n",
      "      decoder Loss: 0.013809125870466232\n",
      "      sindy_z Loss: 66.67465209960938\n",
      "      sindy_x Loss: 15.2256498336792\n",
      "      sindy_regularization Loss: 0.5075920224189758\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.01114719733595848\n",
      "      decoder Loss: 0.010088547132909298\n",
      "      sindy_z Loss: 55.39198303222656\n",
      "      sindy_x Loss: 10.535747528076172\n",
      "      sindy_regularization Loss: 0.5075920224189758\n",
      "Decoder Loss Ratio: 0.054999, Decoder SINDy Loss Ratio: 1.031090\n",
      "Epoch 27\n",
      "   Training Losses:\n",
      "      Total Loss: 0.014594432897865772\n",
      "      decoder Loss: 0.013083639554679394\n",
      "      sindy_z Loss: 63.25186538696289\n",
      "      sindy_x Loss: 15.057512283325195\n",
      "      sindy_regularization Loss: 0.504204511642456\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.010615620762109756\n",
      "      decoder Loss: 0.009573441930115223\n",
      "      sindy_z Loss: 51.654197692871094\n",
      "      sindy_x Loss: 10.371362686157227\n",
      "      sindy_regularization Loss: 0.504204511642456\n",
      "Decoder Loss Ratio: 0.052190, Decoder SINDy Loss Ratio: 1.015002\n",
      "Epoch 28\n",
      "   Training Losses:\n",
      "      Total Loss: 0.014004784636199474\n",
      "      decoder Loss: 0.01250534225255251\n",
      "      sindy_z Loss: 60.64838790893555\n",
      "      sindy_x Loss: 14.944307327270508\n",
      "      sindy_regularization Loss: 0.5011325478553772\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.010223964229226112\n",
      "      decoder Loss: 0.009192352183163166\n",
      "      sindy_z Loss: 48.85325241088867\n",
      "      sindy_x Loss: 10.26600456237793\n",
      "      sindy_regularization Loss: 0.5011325478553772\n",
      "Decoder Loss Ratio: 0.050113, Decoder SINDy Loss Ratio: 1.004691\n",
      "Epoch 29\n",
      "   Training Losses:\n",
      "      Total Loss: 0.013472778722643852\n",
      "      decoder Loss: 0.011987555772066116\n",
      "      sindy_z Loss: 58.53764724731445\n",
      "      sindy_x Loss: 14.802488327026367\n",
      "      sindy_regularization Loss: 0.4974563419818878\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.009870111010968685\n",
      "      decoder Loss: 0.008850731886923313\n",
      "      sindy_z Loss: 46.723175048828125\n",
      "      sindy_x Loss: 10.144052505493164\n",
      "      sindy_regularization Loss: 0.4974563419818878\n",
      "Decoder Loss Ratio: 0.048251, Decoder SINDy Loss Ratio: 0.992756\n",
      "Epoch 30\n",
      "   Training Losses:\n",
      "      Total Loss: 0.013027419336140156\n",
      "      decoder Loss: 0.01155551616102457\n",
      "      sindy_z Loss: 56.562278747558594\n",
      "      sindy_x Loss: 14.669503211975098\n",
      "      sindy_regularization Loss: 0.4953185021877289\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.009583364240825176\n",
      "      decoder Loss: 0.008574877865612507\n",
      "      sindy_z Loss: 44.747222900390625\n",
      "      sindy_x Loss: 10.035335540771484\n",
      "      sindy_regularization Loss: 0.4953185021877289\n",
      "Decoder Loss Ratio: 0.046747, Decoder SINDy Loss Ratio: 0.982117\n",
      "Epoch 31\n",
      "   Training Losses:\n",
      "      Total Loss: 0.01267064455896616\n",
      "      decoder Loss: 0.01121053658425808\n",
      "      sindy_z Loss: 55.2420654296875\n",
      "      sindy_x Loss: 14.55174446105957\n",
      "      sindy_regularization Loss: 0.4933246672153473\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.009360902942717075\n",
      "      decoder Loss: 0.008362277410924435\n",
      "      sindy_z Loss: 43.29512023925781\n",
      "      sindy_x Loss: 9.936923027038574\n",
      "      sindy_regularization Loss: 0.4933246672153473\n",
      "Decoder Loss Ratio: 0.045588, Decoder SINDy Loss Ratio: 0.972485\n",
      "Epoch 32\n",
      "   Training Losses:\n",
      "      Total Loss: 0.012354973703622818\n",
      "      decoder Loss: 0.010905412025749683\n",
      "      sindy_z Loss: 54.16618728637695\n",
      "      sindy_x Loss: 14.44647216796875\n",
      "      sindy_regularization Loss: 0.49143627285957336\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.009164063259959221\n",
      "      decoder Loss: 0.008174053393304348\n",
      "      sindy_z Loss: 42.083961486816406\n",
      "      sindy_x Loss: 9.850951194763184\n",
      "      sindy_regularization Loss: 0.49143627285957336\n",
      "Decoder Loss Ratio: 0.044562, Decoder SINDy Loss Ratio: 0.964072\n",
      "Epoch 33\n",
      "   Training Losses:\n",
      "      Total Loss: 0.012026902288198471\n",
      "      decoder Loss: 0.010586325079202652\n",
      "      sindy_z Loss: 53.92731475830078\n",
      "      sindy_x Loss: 14.357078552246094\n",
      "      sindy_regularization Loss: 0.4869314730167389\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.008956791833043098\n",
      "      decoder Loss: 0.00797374825924635\n",
      "      sindy_z Loss: 41.71074676513672\n",
      "      sindy_x Loss: 9.781749725341797\n",
      "      sindy_regularization Loss: 0.4869314730167389\n",
      "Decoder Loss Ratio: 0.043470, Decoder SINDy Loss Ratio: 0.957299\n",
      "Epoch 34\n",
      "   Training Losses:\n",
      "      Total Loss: 0.01177997887134552\n",
      "      decoder Loss: 0.010346782393753529\n",
      "      sindy_z Loss: 52.5526237487793\n",
      "      sindy_x Loss: 14.283411979675293\n",
      "      sindy_regularization Loss: 0.48547232151031494\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.008802094496786594\n",
      "      decoder Loss: 0.007825309410691261\n",
      "      sindy_z Loss: 40.35623550415039\n",
      "      sindy_x Loss: 9.71929931640625\n",
      "      sindy_regularization Loss: 0.48547232151031494\n",
      "Decoder Loss Ratio: 0.042660, Decoder SINDy Loss Ratio: 0.951188\n",
      "Epoch 35\n",
      "   Training Losses:\n",
      "      Total Loss: 0.011594007723033428\n",
      "      decoder Loss: 0.010168992914259434\n",
      "      sindy_z Loss: 51.25575256347656\n",
      "      sindy_x Loss: 14.201743125915527\n",
      "      sindy_regularization Loss: 0.48403969407081604\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.008691428229212761\n",
      "      decoder Loss: 0.007721484173089266\n",
      "      sindy_z Loss: 39.16011047363281\n",
      "      sindy_x Loss: 9.651037216186523\n",
      "      sindy_regularization Loss: 0.48403969407081604\n",
      "Decoder Loss Ratio: 0.042094, Decoder SINDy Loss Ratio: 0.944507\n",
      "Epoch 36\n",
      "   Training Losses:\n",
      "      Total Loss: 0.011417358182370663\n",
      "      decoder Loss: 0.010000133886933327\n",
      "      sindy_z Loss: 49.99652099609375\n",
      "      sindy_x Loss: 14.123978614807129\n",
      "      sindy_regularization Loss: 0.48261213302612305\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.008584643714129925\n",
      "      decoder Loss: 0.007621001917868853\n",
      "      sindy_z Loss: 38.12307357788086\n",
      "      sindy_x Loss: 9.588154792785645\n",
      "      sindy_regularization Loss: 0.48261213302612305\n",
      "Decoder Loss Ratio: 0.041547, Decoder SINDy Loss Ratio: 0.938353\n",
      "Epoch 37\n",
      "   Training Losses:\n",
      "      Total Loss: 0.011186572723090649\n",
      "      decoder Loss: 0.00977570191025734\n",
      "      sindy_z Loss: 49.02596664428711\n",
      "      sindy_x Loss: 14.060582160949707\n",
      "      sindy_regularization Loss: 0.48117467761039734\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.008433874696493149\n",
      "      decoder Loss: 0.007475385908037424\n",
      "      sindy_z Loss: 37.42184066772461\n",
      "      sindy_x Loss: 9.536766052246094\n",
      "      sindy_regularization Loss: 0.48117467761039734\n",
      "Decoder Loss Ratio: 0.040753, Decoder SINDy Loss Ratio: 0.933324\n",
      "Epoch 38\n",
      "   Training Losses:\n",
      "      Total Loss: 0.011018681339919567\n",
      "      decoder Loss: 0.009613190777599812\n",
      "      sindy_z Loss: 47.839664459228516\n",
      "      sindy_x Loss: 14.006937980651855\n",
      "      sindy_regularization Loss: 0.47973981499671936\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.008326662704348564\n",
      "      decoder Loss: 0.00737242866307497\n",
      "      sindy_z Loss: 36.53544998168945\n",
      "      sindy_x Loss: 9.494368553161621\n",
      "      sindy_regularization Loss: 0.47973981499671936\n",
      "Decoder Loss Ratio: 0.040191, Decoder SINDy Loss Ratio: 0.929175\n",
      "Epoch 39\n",
      "   Training Losses:\n",
      "      Total Loss: 0.010877136141061783\n",
      "      decoder Loss: 0.009477787651121616\n",
      "      sindy_z Loss: 46.70716094970703\n",
      "      sindy_x Loss: 13.945650100708008\n",
      "      sindy_regularization Loss: 0.4782997667789459\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.008236274123191833\n",
      "      decoder Loss: 0.007286857347935438\n",
      "      sindy_z Loss: 35.65977478027344\n",
      "      sindy_x Loss: 9.446335792541504\n",
      "      sindy_regularization Loss: 0.4782997667789459\n",
      "Decoder Loss Ratio: 0.039725, Decoder SINDy Loss Ratio: 0.924474\n",
      "Epoch 40\n",
      "   Training Losses:\n",
      "      Total Loss: 0.010751885361969471\n",
      "      decoder Loss: 0.009359367191791534\n",
      "      sindy_z Loss: 45.65687942504883\n",
      "      sindy_x Loss: 13.877732276916504\n",
      "      sindy_regularization Loss: 0.47451913356781006\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.008156110532581806\n",
      "      decoder Loss: 0.00721212150529027\n",
      "      sindy_z Loss: 34.854156494140625\n",
      "      sindy_x Loss: 9.392438888549805\n",
      "      sindy_regularization Loss: 0.47451913356781006\n",
      "Decoder Loss Ratio: 0.039318, Decoder SINDy Loss Ratio: 0.919199\n",
      "Epoch 41\n",
      "   Training Losses:\n",
      "      Total Loss: 0.01061469316482544\n",
      "      decoder Loss: 0.00922814104706049\n",
      "      sindy_z Loss: 44.79886245727539\n",
      "      sindy_x Loss: 13.8182954788208\n",
      "      sindy_regularization Loss: 0.47230565547943115\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.008065219037234783\n",
      "      decoder Loss: 0.007125913631170988\n",
      "      sindy_z Loss: 34.216796875\n",
      "      sindy_x Loss: 9.345823287963867\n",
      "      sindy_regularization Loss: 0.47230565547943115\n",
      "Decoder Loss Ratio: 0.038848, Decoder SINDy Loss Ratio: 0.914637\n",
      "Epoch 42\n",
      "   Training Losses:\n",
      "      Total Loss: 0.010452459566295147\n",
      "      decoder Loss: 0.00907061342149973\n",
      "      sindy_z Loss: 44.13919448852539\n",
      "      sindy_x Loss: 13.771299362182617\n",
      "      sindy_regularization Loss: 0.4716648757457733\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.00795347336679697\n",
      "      decoder Loss: 0.007017767056822777\n",
      "      sindy_z Loss: 33.75688934326172\n",
      "      sindy_x Loss: 9.309905052185059\n",
      "      sindy_regularization Loss: 0.4716648757457733\n",
      "Decoder Loss Ratio: 0.038258, Decoder SINDy Loss Ratio: 0.911122\n",
      "Epoch 43\n",
      "   Training Losses:\n",
      "      Total Loss: 0.010318364948034286\n",
      "      decoder Loss: 0.008940747939050198\n",
      "      sindy_z Loss: 43.4193000793457\n",
      "      sindy_x Loss: 13.729217529296875\n",
      "      sindy_regularization Loss: 0.4696103632450104\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.007861020043492317\n",
      "      decoder Loss: 0.006928741931915283\n",
      "      sindy_z Loss: 33.21962356567383\n",
      "      sindy_x Loss: 9.27582836151123\n",
      "      sindy_regularization Loss: 0.4696103632450104\n",
      "Decoder Loss Ratio: 0.037773, Decoder SINDy Loss Ratio: 0.907787\n",
      "Epoch 44\n",
      "   Training Losses:\n",
      "      Total Loss: 0.010202995501458645\n",
      "      decoder Loss: 0.008829464204609394\n",
      "      sindy_z Loss: 42.709224700927734\n",
      "      sindy_x Loss: 13.688399314880371\n",
      "      sindy_regularization Loss: 0.4691222012042999\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.007781351450830698\n",
      "      decoder Loss: 0.006852097809314728\n",
      "      sindy_z Loss: 32.691986083984375\n",
      "      sindy_x Loss: 9.245624542236328\n",
      "      sindy_regularization Loss: 0.4691222012042999\n",
      "Decoder Loss Ratio: 0.037355, Decoder SINDy Loss Ratio: 0.904831\n",
      "Epoch 45\n",
      "   Training Losses:\n",
      "      Total Loss: 0.010102170519530773\n",
      "      decoder Loss: 0.008732804097235203\n",
      "      sindy_z Loss: 42.0296630859375\n",
      "      sindy_x Loss: 13.646798133850098\n",
      "      sindy_regularization Loss: 0.46862104535102844\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.007711743004620075\n",
      "      decoder Loss: 0.006785614416003227\n",
      "      sindy_z Loss: 32.18988800048828\n",
      "      sindy_x Loss: 9.2144193649292\n",
      "      sindy_regularization Loss: 0.46862104535102844\n",
      "Decoder Loss Ratio: 0.036992, Decoder SINDy Loss Ratio: 0.901777\n",
      "Epoch 46\n",
      "   Training Losses:\n",
      "      Total Loss: 0.010004620999097824\n",
      "      decoder Loss: 0.008639378473162651\n",
      "      sindy_z Loss: 41.397247314453125\n",
      "      sindy_x Loss: 13.605622291564941\n",
      "      sindy_regularization Loss: 0.46810346841812134\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.007643925026059151\n",
      "      decoder Loss: 0.0067209163680672646\n",
      "      sindy_z Loss: 31.7297420501709\n",
      "      sindy_x Loss: 9.183279037475586\n",
      "      sindy_regularization Loss: 0.46810346841812134\n",
      "Decoder Loss Ratio: 0.036640, Decoder SINDy Loss Ratio: 0.898729\n",
      "Epoch 47\n",
      "   Training Losses:\n",
      "      Total Loss: 0.009878987446427345\n",
      "      decoder Loss: 0.008517765440046787\n",
      "      sindy_z Loss: 40.83450698852539\n",
      "      sindy_x Loss: 13.565469741821289\n",
      "      sindy_regularization Loss: 0.467567503452301\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.007553824223577976\n",
      "      decoder Loss: 0.0066338530741631985\n",
      "      sindy_z Loss: 31.33458709716797\n",
      "      sindy_x Loss: 9.1529541015625\n",
      "      sindy_regularization Loss: 0.467567503452301\n",
      "Decoder Loss Ratio: 0.036165, Decoder SINDy Loss Ratio: 0.895762\n",
      "Epoch 48\n",
      "   Training Losses:\n",
      "      Total Loss: 0.009751933626830578\n",
      "      decoder Loss: 0.008394142612814903\n",
      "      sindy_z Loss: 40.315521240234375\n",
      "      sindy_x Loss: 13.53119945526123\n",
      "      sindy_regularization Loss: 0.4670199751853943\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.00746150640770793\n",
      "      decoder Loss: 0.006544254720211029\n",
      "      sindy_z Loss: 30.968542098999023\n",
      "      sindy_x Loss: 9.125818252563477\n",
      "      sindy_regularization Loss: 0.4670199751853943\n",
      "Decoder Loss Ratio: 0.035677, Decoder SINDy Loss Ratio: 0.893106\n",
      "Epoch 49\n",
      "   Training Losses:\n",
      "      Total Loss: 0.009646772406995296\n",
      "      decoder Loss: 0.008292783983051777\n",
      "      sindy_z Loss: 39.68871307373047\n",
      "      sindy_x Loss: 13.493355751037598\n",
      "      sindy_regularization Loss: 0.4653158485889435\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.00738500477746129\n",
      "      decoder Loss: 0.006470655556768179\n",
      "      sindy_z Loss: 30.496719360351562\n",
      "      sindy_x Loss: 9.096959114074707\n",
      "      sindy_regularization Loss: 0.4653158485889435\n",
      "Decoder Loss Ratio: 0.035275, Decoder SINDy Loss Ratio: 0.890282\n",
      "Epoch 50\n",
      "   Training Losses:\n",
      "      Total Loss: 0.009554357267916203\n",
      "      decoder Loss: 0.008203846402466297\n",
      "      sindy_z Loss: 39.057621002197266\n",
      "      sindy_x Loss: 13.458712577819824\n",
      "      sindy_regularization Loss: 0.4640103578567505\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.00731697166338563\n",
      "      decoder Loss: 0.0064055281691253185\n",
      "      sindy_z Loss: 30.004093170166016\n",
      "      sindy_x Loss: 9.068032264709473\n",
      "      sindy_regularization Loss: 0.4640103578567505\n",
      "Decoder Loss Ratio: 0.034920, Decoder SINDy Loss Ratio: 0.887451\n",
      "REFINEMENT\n",
      "Epoch 0\n",
      "   Training Losses:\n",
      "      Total Loss: 0.006893003825098276\n",
      "      decoder Loss: 0.0062555354088544846\n",
      "      sindy_z Loss: 23.01772689819336\n",
      "      sindy_x Loss: 6.328270435333252\n",
      "      sindy_regularization Loss: 0.46410152316093445\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.007260281126946211\n",
      "      decoder Loss: 0.006351267918944359\n",
      "      sindy_z Loss: 29.57118797302246\n",
      "      sindy_x Loss: 9.043721199035645\n",
      "      sindy_regularization Loss: 0.46410152316093445\n",
      "Decoder Loss Ratio: 0.034625, Decoder SINDy Loss Ratio: 0.885072\n",
      "Epoch 1\n",
      "   Training Losses:\n",
      "      Total Loss: 0.006874747574329376\n",
      "      decoder Loss: 0.006240074522793293\n",
      "      sindy_z Loss: 22.597759246826172\n",
      "      sindy_x Loss: 6.300313472747803\n",
      "      sindy_regularization Loss: 0.464169979095459\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.0072100525721907616\n",
      "      decoder Loss: 0.006303485948592424\n",
      "      sindy_z Loss: 29.171730041503906\n",
      "      sindy_x Loss: 9.01924991607666\n",
      "      sindy_regularization Loss: 0.464169979095459\n",
      "Decoder Loss Ratio: 0.034364, Decoder SINDy Loss Ratio: 0.882677\n",
      "Epoch 2\n",
      "   Training Losses:\n",
      "      Total Loss: 0.006815802771598101\n",
      "      decoder Loss: 0.006183971650898457\n",
      "      sindy_z Loss: 22.23835563659668\n",
      "      sindy_x Loss: 6.271891117095947\n",
      "      sindy_regularization Loss: 0.46422016620635986\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.007138172164559364\n",
      "      decoder Loss: 0.006234114058315754\n",
      "      sindy_z Loss: 28.82471466064453\n",
      "      sindy_x Loss: 8.994159698486328\n",
      "      sindy_regularization Loss: 0.46422016620635986\n",
      "Decoder Loss Ratio: 0.033986, Decoder SINDy Loss Ratio: 0.880221\n",
      "Epoch 3\n",
      "   Training Losses:\n",
      "      Total Loss: 0.006721741519868374\n",
      "      decoder Loss: 0.006092815659940243\n",
      "      sindy_z Loss: 21.923887252807617\n",
      "      sindy_x Loss: 6.242834091186523\n",
      "      sindy_regularization Loss: 0.4642600119113922\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.007049199193716049\n",
      "      decoder Loss: 0.006147620268166065\n",
      "      sindy_z Loss: 28.521451950073242\n",
      "      sindy_x Loss: 8.96936321258545\n",
      "      sindy_regularization Loss: 0.4642600119113922\n",
      "Decoder Loss Ratio: 0.033514, Decoder SINDy Loss Ratio: 0.877794\n",
      "Epoch 4\n",
      "   Training Losses:\n",
      "      Total Loss: 0.00666669150814414\n",
      "      decoder Loss: 0.0060401796363294125\n",
      "      sindy_z Loss: 21.529470443725586\n",
      "      sindy_x Loss: 6.218690395355225\n",
      "      sindy_regularization Loss: 0.4642988443374634\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.006982383318245411\n",
      "      decoder Loss: 0.006082785315811634\n",
      "      sindy_z Loss: 28.136737823486328\n",
      "      sindy_x Loss: 8.949548721313477\n",
      "      sindy_regularization Loss: 0.4642988443374634\n",
      "Decoder Loss Ratio: 0.033161, Decoder SINDy Loss Ratio: 0.875855\n",
      "Epoch 5\n",
      "   Training Losses:\n",
      "      Total Loss: 0.006624647416174412\n",
      "      decoder Loss: 0.006000374909490347\n",
      "      sindy_z Loss: 21.10053062438965\n",
      "      sindy_x Loss: 6.19628381729126\n",
      "      sindy_regularization Loss: 0.46442097425460815\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.006923095788806677\n",
      "      decoder Loss: 0.0060254354029893875\n",
      "      sindy_z Loss: 27.72148323059082\n",
      "      sindy_x Loss: 8.930163383483887\n",
      "      sindy_regularization Loss: 0.46442097425460815\n",
      "Decoder Loss Ratio: 0.032848, Decoder SINDy Loss Ratio: 0.873958\n",
      "Epoch 6\n",
      "   Training Losses:\n",
      "      Total Loss: 0.006593580823391676\n",
      "      decoder Loss: 0.005971599370241165\n",
      "      sindy_z Loss: 20.692556381225586\n",
      "      sindy_x Loss: 6.173306465148926\n",
      "      sindy_regularization Loss: 0.4650976359844208\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.006870469078421593\n",
      "      decoder Loss: 0.005974872969090939\n",
      "      sindy_z Loss: 27.31614112854004\n",
      "      sindy_x Loss: 8.909448623657227\n",
      "      sindy_regularization Loss: 0.4650976359844208\n",
      "Decoder Loss Ratio: 0.032573, Decoder SINDy Loss Ratio: 0.871931\n",
      "Epoch 7\n",
      "   Training Losses:\n",
      "      Total Loss: 0.006580809131264687\n",
      "      decoder Loss: 0.0059611983597278595\n",
      "      sindy_z Loss: 20.313045501708984\n",
      "      sindy_x Loss: 6.149532318115234\n",
      "      sindy_regularization Loss: 0.46577349305152893\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.006829524412751198\n",
      "      decoder Loss: 0.005936139728873968\n",
      "      sindy_z Loss: 26.930938720703125\n",
      "      sindy_x Loss: 8.887269020080566\n",
      "      sindy_regularization Loss: 0.46577349305152893\n",
      "Decoder Loss Ratio: 0.032361, Decoder SINDy Loss Ratio: 0.869760\n",
      "Epoch 8\n",
      "   Training Losses:\n",
      "      Total Loss: 0.0065814414992928505\n",
      "      decoder Loss: 0.005964267998933792\n",
      "      sindy_z Loss: 19.974979400634766\n",
      "      sindy_x Loss: 6.125088214874268\n",
      "      sindy_regularization Loss: 0.46645066142082214\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.006797275505959988\n",
      "      decoder Loss: 0.005906245205551386\n",
      "      sindy_z Loss: 26.57879638671875\n",
      "      sindy_x Loss: 8.863656997680664\n",
      "      sindy_regularization Loss: 0.46645066142082214\n",
      "Decoder Loss Ratio: 0.032198, Decoder SINDy Loss Ratio: 0.867449\n",
      "Epoch 9\n",
      "   Training Losses:\n",
      "      Total Loss: 0.006542975082993507\n",
      "      decoder Loss: 0.005928334314376116\n",
      "      sindy_z Loss: 19.696687698364258\n",
      "      sindy_x Loss: 6.099694728851318\n",
      "      sindy_regularization Loss: 0.4671291708946228\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.0067432415671646595\n",
      "      decoder Loss: 0.005854724906384945\n",
      "      sindy_z Loss: 26.275005340576172\n",
      "      sindy_x Loss: 8.838451385498047\n",
      "      sindy_regularization Loss: 0.4671291708946228\n",
      "Decoder Loss Ratio: 0.031918, Decoder SINDy Loss Ratio: 0.864983\n",
      "Epoch 10\n",
      "   Training Losses:\n",
      "      Total Loss: 0.006440439727157354\n",
      "      decoder Loss: 0.005828654859215021\n",
      "      sindy_z Loss: 19.47168731689453\n",
      "      sindy_x Loss: 6.071067810058594\n",
      "      sindy_regularization Loss: 0.4678123891353607\n",
      "   Validation Losses:\n",
      "      Total Loss: 0.006657451391220093\n",
      "      decoder Loss: 0.005771683529019356\n",
      "      sindy_z Loss: 26.01212501525879\n",
      "      sindy_x Loss: 8.810896873474121\n",
      "      sindy_regularization Loss: 0.4678123891353607\n",
      "Decoder Loss Ratio: 0.031465, Decoder SINDy Loss Ratio: 0.862286\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_experiments = 1\n",
    "df = pd.DataFrame()\n",
    "for i in range(num_experiments):\n",
    "    print('EXPERIMENT %d' % i)\n",
    "\n",
    "    params['coefficient_mask'] = torch.ones((params['library_dim'], params['latent_dim'])).to(\"cuda:0\")\n",
    "\n",
    "    params['save_name'] = 'lorenz_' + datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S_%f\")\n",
    "\n",
    "    results_dict = train_network(training_data, validation_data, params)\n",
    "    # print(type(df))\n",
    "    # df = df.concat({**results_dict, **params}, ignore_index=True)\n",
    "    df = pd.concat([df, pd.DataFrame([{**results_dict, **params}])], ignore_index=True)\n",
    "\n",
    "df.to_pickle('experiment_results_' + datetime.datetime.now().strftime(\"%Y%m%d%H%M\") + '.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
